INSTALANDO SPARK

##Para atualizacao do ubuntu
sudo apt update
sudo apt -y upgrade

##Para instalar o Java
sudo apt install curl mlocate default-jdk -y

##Para instalar o SPARK
#colocar no google download spark
#copiar o link de download 
wget <colar link>
ls
#extrair arquivo baixado
tar xvf spark.... 
#mover arquivo para pasta /opt/spark
sudo mv spark-3.3.2-bin-hadoop3/ /opt/spark

#definir variaveis de ambiente
sudo gedit ~/.bashrc
#vai para o final do arquivo inclui variaveis de ambiente e salva
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
#atualiza 
source ~/.bashrc

#inicializa SPARK
start-master.sh

##########################
## Utilizando PYSPARK   ##
##########################

##
####################################################################
## 1. DATAFRAMES
##    
####################################################################
##

from pyspark.sql.types import *
##Definindo schema e suas colunas
arqschema = "id INT, nome STRING, status STRING, cidade STRING, vendas INT, data TIMESTAMP"

##Importando dados de arquivo csv e atribuindo schema arqschema
despachantes = spark.read.csv("/home/ariadne/download/despachantes.csv", header=False, schema=arqschema)
##Mostra dados do arquivo despachantes
despachantes.show()
##Mostra o schema do despachantes
despachantes.schema



##Exportando dados despachantes para formato parquet,csv,json e orc
despachantes.write.format("parquet").save("/home/ariadne/dfimportparquet")
despachantes.write.format("csv").save("/home/ariadne/dfimportcsv")
despachantes.write.format("json").save("/home/ariadne/dfimportjson")
despachantes.write.format("orc").save("/home/ariadne/dfimportorc")


##Renomear arquivos no terminal linux (abrir nova janela pra nao sair do pyspark)
##entrar na pasta onde o arquivo se encontra ex: cd dfimportparquet
cd <nome da pasta>
ls
mv <nome arquivo> despachantes.csv
mv <nome arquivo> despachantes.json
mv <nome arquivo> despachantes.parquet
mv <nome arquivo> despachantes.orc

##Importando dados despachantes do formato parquet
par = spark.read.format("parquet").load("/home/ariadne/dfimportparquet/despachantes.parquet")
par.show()

par.schema

##Importando dados despachantes do formato json
js = spark.read.format("json").load("/home/ariadne/dfimportjson/despachantes.json")
js.show()

js.schema

##Importando dados despachantes do formato orc
orc = spark.read.format("orc").load("/home/ariadne/dfimportorc/despachantes.orc")
orc.show()

orc.schema

##Importando dados despachantes do formato csv
csv = spark.read.format("csv").load("/home/ariadne/dfimportcsv/despachantes.csv", schema=arqschema)
csv.show()

csv.schema

##ver o schema que foi criado anteriormente.
arqschema



####################################################################
## DESAFIO:
## 1. Criar consulta que mostre nesta ordem: Nome, Estados e Status
##    
####################################################################
from pyspark.sql.types import *

##importando dados no formato parquet
Clientes = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Clientes.parquet")
Vendas = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Vendas.parquet")
Vendedores = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Vendedores.parquet")
ItensVendas = spark.read.format("parquet").load("/home/ariadne/download/Atividades/ItensVendas.parquet")
Produtos = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Produtos.parquet")

Atividade1 = Clientes.select("Cliente","Estado", "Status")
Atividade1.show()

####################################################################
## DESAFIO:
## 2. Criar consulta que mostre apenas os clientes do Status:
##    "Platinum" e "Gold"
####################################################################
from pyspark.sql import functions as Func

Atividade2 = Clientes.select("*").where((Func.col("Status")=="Platinum")|(Func.col("Status")=="Gold"))
Atividade2.show()

####################################################################
## DESAFIO:
## 3. Demonstre quanto cada status de clientes representa em vendas
##    
####################################################################
from pyspark.sql.functions import sum as SUM

Atividade3 = Vendas.join(Clientes, Vendas.ClienteID == Clientes.ClienteID).groupBy(Clientes.Status).agg(SUM("Total")).orderBy(Func.col("sum(Total)").desc())
Atividade3.show()




##
####################################################################
## 2. SPARK SQL
##    
####################################################################
##

from pyspark.sql import SparkSession
from pyspark.sql.types import *

##Mostra todos os databases existentes
spark.sql("show databases").show()

##Cria um database chamado desp
spark.sql("create database desp")

##Seleciona o database desp para execução das proximas atividades
spark.sql("use desp").show()

##Define para arqschema as suas colunas e tipo
arqschema = "id INT, nome STRING, status STRING, cidade STRING, vendas INT, data TIMESTAMP"
##Importando dados de arquivo csv e atribuindo schema arqschema
despachantes = spark.read.csv("/home/ariadne/download/despachantes.csv", header=False, schema=arqschema)

despachantes.show()

##Cria a tabela gerenciada despachantes no database desp
despachantes.write.saveAsTable("Despachantes")
##Faz um select na cabela que foi criada
spark.sql("select * from Despachantes").show()
##Mostra todas as tabelas que o database desp possui
spark.sql("show tables").show()

##Sobrescreve a tabela despachantes no database desp
despachantes.write.mode("overwrite").saveAsTable("Despachantes")




##SAIR DO TERMINAL PYSPARK CTRL+D ##

##Mostra todos os databases existentes
spark.sql("show databases").show()

##Seleciona o database desp para execução das proximas atividades
spark.sql("use desp").show()

##Recriar dataframe despachante a partir da tabela despachantes
despachantes = spark.sql("select * from Despachantes")
despachantes.show()



##Criar tabela Não gerenciável ##

##Gerar arquivo parquet na pasta desparquet
despachantes.write.format("parquet").save("/home/ariadne/desparquet")
##Criar tabela não gerenciável a partir de um arquivo parquet localizado na pasta desparquet
despachantes.write.option("path","/home/ariadne/desparquet/despachantes.parquet").saveAsTable("Despachantes_ng")

spark.sql("select * from Despachantes_ng").show()


##Mostra comando de criação da tabela da tabela gerenciavel
spark.sql("show create table despachantes").show(truncate=False)
##Mostra comando de criação da tabela da tabela Não gerenciavel (EXTERNA)
spark.sql("show create table despachantes_ng").show(truncate=False)

##Lista as tabelas
spark.catalog.listTables()

##OBS: os arquivos EXTERNOS ficam armazenados da pasta spark-warehouse ##




## VIEWS ##

##Criar views temporárias ##
despachantes.createOrReplaceTempView("Despachantes_view1")
spark.sql("select * from Despachantes_view1").show()

##Criar views globais ##
despachantes.createOrReplaceGlobalTempView("Despachantes_view2")
spark.sql("select * from global_temp.Despachantes_view2").show()

##Criar views temporárias com comando SQL##
spark.sql("CREATE OR REPLACE TEMP VIEW DESP_VIEW AS select * from despachantes")
spark.sql("select * from DESP_VIEW").show()

##Criar views Global com comando SQL##
spark.sql("CREATE OR REPLACE GLOBAL TEMP VIEW DESP_VIEW2 AS select * from despachantes")
spark.sql("select * from global_temp.DESP_VIEW2").show()

##Apagar view Global com comando SQL##
spark.sql("DROP VIEW global_temp.DESP_VIEW2")




## COMPARANDO DATAFRAMES COM TABELAS ##

from pyspark.sql import functions as Func
from pyspark.sql.functions import *
#SQL - consulta Tabela
spark.sql("select * from Despachantes").show()
#API - Consulta DataFrame
despachantes.show()

#SQL - consulta Tabela
spark.sql("select nome, vendas from Despachantes").show()
#API - Consulta DataFrame
despachantes.select("nome", "vendas").show()

#SQL - consulta Tabela
spark.sql("select nome, vendas from Despachantes where vendas > 20").show()
#API - Consulta DataFrame
despachantes.select("nome", "vendas").where(Func.col("vendas")>20).show()

#SQL - consulta Tabela
spark.sql("select nome, sum(vendas) from Despachantes group by cidade order by 2 desc").show()
#API - Consulta DataFrame
despachantes.groupBy("cidade").agg(sum("vendas")).orderBy(Func.col("sum(vendas)").desc()).show()



## JOINS ##

##criar schema e dataframe reclamações
recschema = "idrec INT, datarec STRING, iddesp INT"
reclamacoes = spark.read.csv("/home/ariadne/download/reclamacoes.csv", header=False, schema=recschema)
reclamacoes.show()

##criar tabela no database
reclamacoes.write.saveAsTable("reclamacoes")

##inner join com SQL - consulta Tabela
spark.sql("select reclamacoes.*,despachantes.nome from despachantes inner join reclamacoes on(despachantes.id = reclamacoes.iddesp)").show()
##Right join com SQL - consulta Tabela
spark.sql("select reclamacoes.*,despachantes.nome from despachantes right join reclamacoes on(despachantes.id = reclamacoes.iddesp)").show()
##Left join com SQL - consulta Tabela
spark.sql("select reclamacoes.*,despachantes.nome from despachantes left join reclamacoes on(despachantes.id = reclamacoes.iddesp)").show()


##inner join com API - Consulta DataFrame
despachantes.join(reclamacoes, despachantes.id == reclamacoes.iddesp, "inner").select("idrec","datarec","nome").show()
##Right join com API - Consulta DataFrame
despachantes.join(reclamacoes, despachantes.id == reclamacoes.iddesp, "right").select("idrec","datarec","nome").show()
##Left join com API - Consulta DataFrame
despachantes.join(reclamacoes, despachantes.id == reclamacoes.iddesp, "left").select("idrec","datarec","nome").show()



##########################
## Utilizando SPARK-SQL ##
##########################

##Mostrar os databases existentes
show databases;
##Definir o database que será utilizado
use desp;
##Mostrar as tabelas existentes no database selecionado
show tables;

##Exemplo de consultas sql executadas no terminal.
select * from despachantes;
select nome, vendas from despachantes;
select nome, vendas from despachantes where vendas > 20;
select cidade, sum(vendas) from despachantes group by cidade;
select reclamacoes.*, despachantes.nome from despachantes inner join reclamacoes on (despachantes.id = reclamacoes.iddesp);
select reclamacoes.*, despachantes.nome from despachantes right join reclamacoes on (despachantes.id = reclamacoes.iddesp);
select reclamacoes.*, despachantes.nome from despachantes left join reclamacoes on (despachantes.id = reclamacoes.iddesp);


####################################################################
## DESAFIO:
## 1. Criar banco de dados no DW do spark chamado VendasVarejo, e 
##    persista todas as tabelas neste banco de dados.
####################################################################
##Criar database VendasVarejo
spark.sql("create database VendasVarejo")
spark.sql("use VendasVarejo")

##importando dados no formato parquet
clientes = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Clientes.parquet")
vendas = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Vendas.parquet")
vendedores = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Vendedores.parquet")
itensVendas = spark.read.format("parquet").load("/home/ariadne/download/Atividades/ItensVendas.parquet")
produtos = spark.read.format("parquet").load("/home/ariadne/download/Atividades/Produtos.parquet")

##Criar tabelas no DW VendasVarejo
clientes.write.saveAsTable("clientes")
vendas.write.saveAsTable("vendas")
vendedores.write.saveAsTable("vendedores")
itensVendas.write.saveAsTable("itensVendas")
produtos.write.saveAsTable("produtos")

##Mostrar as tabelas criadas
spark.sql("show tables").show()

##Mostrar todos os dados da tabela produtos
spark.sql("select * from produtos").show()


####################################################################
## DESAFIO:
## 2. Criar consulta que mostre cada item vendido o: Nome do Cliente,
##    Data de Venda, Produto, Vendedor e Valor total do item
####################################################################

##Seleciona o database desp para execução das proximas atividades
use VendasVarejo;


select  c.cliente, 
        v.data, 
        p.produto, 
        vd.vendedor, 
        iv.valortotal
from itensVendas iv
inner join produtos p on (p.produtoid = iv.produtoid)
inner join vendas v on (v.vendasid = iv.vendasid)
inner join vendedores vd on (vd.vendedorid = v.vendedorid)
inner join clientes c on (c.clienteid = v.clienteid);

##Verifica quantidade de registros 
select count(*) from itensVendas;

du -sh /var/cache/apt/archives



##########################
## Utilizando MONGODB   ##
##########################

##importar database posts de arquivo JSON
mongoimport --db posts --collection post --legacy --file /home/ariadne/download/mongo/posts.json

##Conectar no MONGODB
mongosh

#mostrar databases
show dbs
#selecionar databases
use posts
#mostrar tdos os documentos
db.post.find()

#instalar conector monto to spark
 pyspark --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1
 ##Ler com dataframe base poosts do mongo
 posts = spark.read.format("mongo").option("uri","mongodb://127.0.0.1/posts.post").load()
posts.show()

#Utilizando dataframe posts gravei no mongodb o posts2
posts.write.format("mongo").option("uri","mongodb://127.0.0.1/posts2.post").save()


##Criar Aplicativo 1 - Escrevendo no console##
import csv
import os

#Criar a linha com registros que serão inseridos no arquivo destino
def criar_linha(parceiro, cod_loja, descricao)
    return[f"COD_PARCEIRO;{parceiro};7;11;1;{cod_loja};CANAIS DIGITAIS;{descricao}"]
    

def extrair():
    linhas = []
    #Abrir arquivo origem e identificar campos necessários.
    with open("nosLoja.csv",'r') as arq_origem:
        csv_origem = csv.reader(arq_origem, delimiter=';')
        next(csv_origem)
        
        for coluna in csv_origem:
            parceiro = coluna[0]
            cod_loja = coluna[1]
            descricao = coluna[2]
        
        #Verifica se arquivo está vazio        
        if os.stat("mis_dp_familia_cnl.csv").st_size == 0:
            linhas.append(criar_linha(parceiro, cod_loja, descricao)
        else:
            tratar_registros_destino(cod_loja, descricao, linhas, parceiro)
            
    #Abrir arquivo e incluir linha com registros
    with open("mis_dp_familia_cnl.csv",'a') as arq_destino:
        writer_object = csv.writer(arq_destino)
        for linha in linhas:
            writer_object.writerow(linha)
            
    print(f'Execução finalizada com sucesso. Linhas inseridas: {len(linhas)}')
    

def tratar_registros_destino(cod_loja, descricao,linhas, parceiro):
    novo_registro = True
    arq_destino = open("mis_dp_familia_cnl.csv", 'r')
    csv_destino = csv.reader(arq_destino, delimiter=';')
    
    for coluna in csv_destino:
        if coluna[2] == parceiro and coluna[6] == cod_loja:
            novo_registro = False
    
    if novo_registro:
        linhas.append(criar_linha(parceiro, cod_loja, descricao))
    arq_destino.close()
    

extrair()
